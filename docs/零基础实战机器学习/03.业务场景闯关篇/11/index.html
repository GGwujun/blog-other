<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-other/umi.css" />
    <script>
      window.routerBase = "/blog-other";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>
      15｜二元分类：怎么预测用户是否流失？从逻辑回归到深度学习 - 大师兄
    </title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/零基础实战机器学习/03.业务场景闯关篇/11" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-other/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>前端开发<ul><li><a href="/blog-other/说透低代码">说透低代码</a></li><li><a href="/blog-other/反爬虫兵法演绎20讲">反爬虫兵法演绎20讲</a></li></ul></span><span>产品与用户体验<ul><li><a href="/blog-other/大厂广告产品心法">大厂广告产品心法</a></li></ul></span><span>面试<ul><li><a href="/blog-other/技术面试官识人手册">技术面试官识人手册</a></li><li><a href="/blog-other/面试现场">面试现场</a></li></ul></span><span>杂谈<ul><li><a href="/blog-other/乔新亮的cto成长复盘">乔新亮的cto成长复盘</a></li><li><a href="/blog-other/互联网人的英语私教课">互联网人的英语私教课</a></li><li><a href="/blog-other/从0开始学游戏开发">从0开始学游戏开发</a></li><li><a href="/blog-other/全栈工程师修炼指南">全栈工程师修炼指南</a></li><li><a href="/blog-other/手机摄影">手机摄影</a></li><li><a href="/blog-other/物联网开发实战">物联网开发实战</a></li><li><a href="/blog-other/白话法律42讲">白话法律42讲</a></li><li><a href="/blog-other/说透5g">说透5g</a></li><li><a href="/blog-other/超级访谈对话张雪峰">超级访谈对话张雪峰</a></li><li><a aria-current="page" class="active" href="/blog-other/零基础实战机器学习">零基础实战机器学习</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-other/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>前端开发<ul><li><a href="/blog-other/说透低代码">说透低代码</a></li><li><a href="/blog-other/反爬虫兵法演绎20讲">反爬虫兵法演绎20讲</a></li></ul></li><li>产品与用户体验<ul><li><a href="/blog-other/大厂广告产品心法">大厂广告产品心法</a></li></ul></li><li>面试<ul><li><a href="/blog-other/技术面试官识人手册">技术面试官识人手册</a></li><li><a href="/blog-other/面试现场">面试现场</a></li></ul></li><li>杂谈<ul><li><a href="/blog-other/乔新亮的cto成长复盘">乔新亮的cto成长复盘</a></li><li><a href="/blog-other/互联网人的英语私教课">互联网人的英语私教课</a></li><li><a href="/blog-other/从0开始学游戏开发">从0开始学游戏开发</a></li><li><a href="/blog-other/全栈工程师修炼指南">全栈工程师修炼指南</a></li><li><a href="/blog-other/手机摄影">手机摄影</a></li><li><a href="/blog-other/物联网开发实战">物联网开发实战</a></li><li><a href="/blog-other/白话法律42讲">白话法律42讲</a></li><li><a href="/blog-other/说透5g">说透5g</a></li><li><a href="/blog-other/超级访谈对话张雪峰">超级访谈对话张雪峰</a></li><li><a aria-current="page" class="active" href="/blog-other/零基础实战机器学习">零基础实战机器学习</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-other/零基础实战机器学习">零基础实战机器学习</a></li><li><a href="/blog-other/零基础实战机器学习/01.开篇词">01.开篇词</a><ul><li><a href="/blog-other/零基础实战机器学习/01.开篇词/01"><span>开篇词｜开发者为什么要从实战出发学机器学习？</span></a></li></ul></li><li><a href="/blog-other/零基础实战机器学习/02.准备篇">02.准备篇</a><ul><li><a href="/blog-other/零基础实战机器学习/02.准备篇/01"><span>01｜打好基础：到底什么是机器学习？</span></a></li><li><a href="/blog-other/零基础实战机器学习/02.准备篇/02"><span>02｜工具准备：安装并使用Jupyter Notebook</span></a></li><li><a href="/blog-other/零基础实战机器学习/02.准备篇/03"><span>03｜实战5步（上）：怎么定义问题和预处理数据？</span></a></li><li><a href="/blog-other/零基础实战机器学习/02.准备篇/04"><span>04｜ 实战5步（下）：怎么建立估计10万+软文点击率的模型？</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇">03.业务场景闯关篇</a><ul><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/01"><span>05 | 数据探索：怎样从数据中找到用户的RFM值？</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/02"><span>06 | 聚类分析：如何用RFM给电商用户做价值分组画像？</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/03"><span>07｜回归分析：怎样用模型预测用户的生命周期价值？</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/04"><span>08 | 模型优化（上）：怎么用特征工程提高模型效率？</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/05"><span>09｜模型优化（中）：防止过拟合，模型也不能太精细</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/06"><span>10｜模型优化（下）：交叉验证，同时寻找最优的参数</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/07"><span>11｜深度学习（上）：用CNN带你认识深度学习</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/08"><span>12｜深度学习（中）：如何用RNN预测激活率走势？</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/09"><span>13｜深度学习（下）：3招提升神经网络预测准确率</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/10"><span>14｜留存分析：哪些因素会影响用户的留存率？</span></a></li><li><a aria-current="page" class="active" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11"><span>15｜二元分类：怎么预测用户是否流失？从逻辑回归到深度学习</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/12"><span>16｜性能评估：不平衡数据集应该使用何种评估指标？</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/13"><span>17｜集成学习：机器学习模型如何“博采众长”?</span></a></li><li><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/14"><span>18 | 增长模型：用XGBoost评估裂变海报的最佳受众群体</span></a></li></ul></li><li><a href="/blog-other/零基础实战机器学习/04.持续赋能篇">04.持续赋能篇</a><ul><li><a href="/blog-other/零基础实战机器学习/04.持续赋能篇/01"><span>19 | 胸有成竹：如何快速定位合适的机器学习算法？</span></a></li><li><a href="/blog-other/零基础实战机器学习/04.持续赋能篇/02"><span>20 | 模型部署：怎么发布训练好的机器学习模型？</span></a></li><li><a href="/blog-other/零基础实战机器学习/04.持续赋能篇/03"><span>21｜持续精进：如何在机器学习领域中找准前进的方向？</span></a></li></ul></li><li><a href="/blog-other/零基础实战机器学习/05.结束语">05.结束语</a><ul><li><a href="/blog-other/零基础实战机器学习/05.结束语/01"><span>一套习题，测出你对机器学习的掌握程度</span></a></li><li><a href="/blog-other/零基础实战机器学习/05.结束语/02"><span>结束语 | 可以不完美，但重要的是马上开始</span></a></li></ul></li><li><a href="/blog-other/零基础实战机器学习/summary">零基础实战机器学习</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"><li title="定义问题" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#定义问题"><span>定义问题</span></a></li><li title="数据预处理" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#数据预处理"><span>数据预处理</span></a></li><li title="算法选择" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#算法选择"><span>算法选择</span></a></li><li title="用逻辑回归解决二元分类问题" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#用逻辑回归解决二元分类问题"><span>用逻辑回归解决二元分类问题</span></a></li><li title="用神经网络解决二元分类问题" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#用神经网络解决二元分类问题"><span>用神经网络解决二元分类问题</span></a></li><li title="归一化之后重新训练神经网络" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#归一化之后重新训练神经网络"><span>归一化之后重新训练神经网络</span></a></li><li title="总结一下" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#总结一下"><span>总结一下</span></a></li><li title="思考题" data-depth="2"><a href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#思考题"><span>思考题</span></a></li></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="15二元分类怎么预测用户是否流失从逻辑回归到深度学习"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#15二元分类怎么预测用户是否流失从逻辑回归到深度学习"><span class="icon icon-link"></span></a>15｜二元分类：怎么预测用户是否流失？从逻辑回归到深度学习</h1><p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>在上一讲中，我们用lifelines包中的工具，在“易速鲜花”的会员信息中，挖掘出了与用户流失相关比性较大的几个因素。今天，运营部门又来了新需求，我们通过这个需求，一起来看看怎么解决二元分类问题。</p><p>之前，我们接触的绝大多数业务场景都是回归场景，但是，后面更多的场景实战中，也需要分类算法大显身手，而今天这一讲将为我们解决后续诸多分类问题打下基础，它的重要性不言而喻。</p><p>好，让我们直接开始吧！</p><h2 id="定义问题"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#定义问题"><span class="icon icon-link"></span></a>定义问题</h2><p>这回运营部门又提出了什么需求呢？运营部门的同事是这样向你诉苦的：“要留住会员，真的是难上加难。老板要求我们啊，对于每一个流失的客户，都要打电话，给优惠来挽留，还要发一个调查问卷，收集为什么他不再续费的原因，你说这不是事后诸葛亮吗？人都走了，挽留还有什么意思呢？你们数据这块能不能给建立一个模型，预测一下哪些客户流失风险比较高，然后我们可以及时触发留客机制，你看行吗？”</p><p>以你现在对机器学习的理解，你觉得这个需求可以做到吗？当然可以。现在，让我们先来回顾一下运营部门给我们的<a target="_blank" rel="noopener noreferrer" href="https://github.com/huangjia2019/geektime/tree/main/%E7%95%99%E5%AD%98%E5%85%B315">这个数据集<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>。</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimagec48ac44ac391fb773b147bd5e80f2c3d678a.a73df6b5.png" alt=""/></p><p>根据运营同事的描述，我们要预测的标签就是“已停付会费”这个字段。从已有的数据中训练出模型，我们自然就可以推知其它具有类似特征的会员，“停付会费”的可能性大小。</p><p>既然有标签，这肯定是一个监督学习问题。再进一步，那它是回归问题，还是分类问题呢？这就要看标签是连续值还是离散值了。“是否已停付会费”这个字段的值，要么为“是”，要么为“否”，也就是非1即0，自然是离散的。所以，这是一个分类问题，而且它还是一个典型的二元分类问题。</p><p>因此，机器学习中的分类模型可以告诉我们每一个用户具体的流失风险。如果风险高，那这个用户很有可能会流失，他就需要被运营团队关注了。请你注意，这个“高风险值”是多个特征相组合显现出的结果，并不是单纯取决于某个特征。</p><p>分析到这里，我们就开始着手处理了。</p><h2 id="数据预处理"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#数据预处理"><span class="icon icon-link"></span></a>数据预处理</h2><p>对于这个问题来说，数据的读入、清洗、可视化和特征工程等工作，我们在上一讲中已经做好了：</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage037c0346762e6ae2079b006159acf6d2ee7c.57159a3b.png" alt=""/></p><p>所以，我们就直接来构建特征集和标签集就可以了：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">X = df_churn.drop([&#x27;Churn&#x27;], axis = 1)   # 基于df_churn构建特征集</span></div><div class="token-line"><span class="token plain">    y = df_churn.Churn.values # 基于df_churn构建标签集</span></div></pre></div><p>然后，我们再来拆分一下训练集和测试集。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">from sklearn.model_selection import train_test_split #导入train_test_split模块</span></div><div class="token-line"><span class="token plain">    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2) #拆分数据集</span></div></pre></div><p>准备好训练集和测试集之后，我们直接进入算法选择的环节。</p><h2 id="算法选择"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#算法选择"><span class="icon icon-link"></span></a>算法选择</h2><p>我们刚才说，“预测哪些客户流失风险比较高”是一个二元分类问题。那么，能够解决这种分类问题的算法又有哪些呢？在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/417479">第7讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中，我们在做回归分析时，介绍了很多回归算法。其实，和回归类似，机器学习中能够用来解决分类问题的算法也非常多，我把比较常用的分类算法整理在下面这个表中：</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage33233340aa5b75faa25dd717f288f1614f23.47a8d7a4.jpg" alt=""/></p><p>我们说过，在解决具体问题的时候，我们通常会选择多种算法进行建模，相互比较之后，再确定比较适合的模型。不过，在这一讲中，我们不太可能把上面所有的模型都挨个讲解和尝试一遍，所以，我就挑两个没讲过的算法：逻辑回归和神经网络，来带你解决这个问题。如果你对其他模型的效果很感兴趣，可以自行做个尝试。</p><p>我们先来看逻辑回归算法。</p><h2 id="用逻辑回归解决二元分类问题"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#用逻辑回归解决二元分类问题"><span class="icon icon-link"></span></a>用逻辑回归解决二元分类问题</h2><p>逻辑回归是最为基础的分类算法，它在分类算法中的地位和和线性回归在回归算法中的地位一样，也常常作为基准算法，其它算法的结果可以与逻辑回归算法进行比较。</p><p>其实，逻辑回归的本质仍然是线性回归，这也是为什么它的名字中仍然保留了“回归”二字。只不过，在线性回归算法的基础之上，它又增加了一个Sigmoid函数。</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage0adf0a4d5bdb93yy537e32f8f99067c199df.a8dc2b8f.png" alt=""/></p><p>这个函数的作用是什么呢？它其实是在线性回归算法的预测值基础上，把预测值做一个非线性的转换，也就是转换成0~1区间的一个值。而这个值，就是逻辑回归算法预测出的分类概率。这个过程你可以参考下面的Sigmoid函数图像以及它的公式。</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage581b58f1c7800c6d30678b9779ea8e3d761b.715d852d.jpg" alt="" title="Sigmiod函数的图像和公式"/></p><p>我们以会员是否流失为例，如果Sigmoid函数转化之后的预测结果值为0.7，就说明流失的概率大于0.5，可以认为该会员可能会流失。如果Sigmoid函数转化之后的预测结果值为0.4，就说明该会员留存的可能性比较高。对于其它的二元分类判断，比如病患是否患病、客户是否存在欺诈行为等等，都是同样的道理。</p><p>明白了这一点后，我们就用逻辑回归算法来预测一下“易速鲜花”的哪些客户流失风险比较高。首先，我们导入逻辑回归算法，并创建逻辑回归模型，我把模型命名为logreg（即LogisticRegression的缩写）：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">from sklearn.linear_model import LogisticRegression #导入逻辑回归模型</span></div><div class="token-line"><span class="token plain">    logreg = LogisticRegression() # lr,就代表是逻辑回归模型</span></div></pre></div><p>然后，我们通过fit方法，开展对机器的训练：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">logreg.fit(X_train,y_train) #拟合模型</span></div></pre></div><p>模型拟合好之后，我们就可以对模型的分数进行评估了。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">print(&quot;SK-learn逻辑回归测试准确率{:.2f}%&quot;.format(logreg.score(X_test,y_test)*100))   #模型分数</span></div></pre></div><p>注意，这里的score方法给出的是预测准确率的均值。</p><p>输入如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">SK-learn逻辑回归测试准确率78.70%</span></div></pre></div><p>结果显示，这个逻辑回归模型在测试集上的准确率为78.70%。</p><p>最后，我们用这个模型来预测具体的用户是否会流失，我们选择测试集的第一个用户查看结果。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">y_pred = logreg.predict(X_test) #对测试集进行预测</span></div><div class="token-line"><span class="token plain">    print(&quot;测试集第一个用户预测结果&quot;, y_pred[0]) #第一个用户预测结果</span></div></pre></div><p>输出如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">测试集第一个用户预测结果 0</span></div></pre></div><p>我们得到的结果是0，说明这个逻辑回归模型判断第一个用户并不会流失，这个预测结果与真值一致。</p><p>这个模型看起来好像还不错，我们是不是可以把它应用于会员流失风险的评估呢？先别急，我们再来看看神经网络模型的表现如何。</p><h2 id="用神经网络解决二元分类问题"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#用神经网络解决二元分类问题"><span class="icon icon-link"></span></a>用神经网络解决二元分类问题</h2><p>你可能还在奇怪，我们已经用神经网络模型解决过问题了，为什么这里还要选择它呢？没错，在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/420372">第11讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>和<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/421029">第12讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中，我们用CNN完成了图像识别，用RNN完成了点击量的预测。不过，我们还没有使用普通的神经网络模型来解决分类问题。</p><p>相对于逻辑回归以及其它的分类算法，神经网络适合解决特征数量比较多、数据集样本数量庞大的分类问题，因为神经网络结构复杂，它的拟合能力当然也就比较强。所以，神经网络是我们解决分类问题时一个不错的选择。</p><p>怎么理解呢？这要从最早的神经网络说起。1958年，Rosenblatt提出了一种一元的“感知器”（Perceptron）模型，这是一种单个神经元的神经网络，也是最基本、最简单的神经网络，它的结构如下图所示：</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage1ff71ff7808c6e6cde196e7e0f9ce4f9a0f7.567b4723.jpg" alt="" title="一元的感知器和逻辑回归模型完全相同"/></p><p>你看，这种一元的感知器从整体上是先接收特征的输入，然后做线性回归，进而通过Sigmoid函数求出分类概率，最后转化为分类值。 它实际上就是一个逻辑回归模型。所以，你可以认为，<strong>最基本、最简单的神经网络就是逻辑回归模型</strong>**。**</p><p>当然，现代神经网络已经演化成了具有很多层的深度学习网络，每一层也有非常多的类型，它们能够解决的问题也就更复杂了。但是，无论是浅层神经网络，还是深层神经网络，它们的网络结构都可以细分并简化为多个“一元感知器”，所以，现代的神经网络也都能够很好地解决二元分类问题。</p><p>现在回到我们的项目，请你思考一个问题，既然我们要预测一下“易速鲜花”的哪些客户流失风险比较高，那什么样的神经网络模型比较合适呢？我们之前学过CNN和RNN，它们合适吗？我们说，CNN网络主要用来处理图形图像等计算机视觉问题，RNN网络主要是处理自然语言、文字和时序问题。而我们当前这个数据具有很好的特征结构，它不是图片、文本，也不是时序数据，因此，它不需要CNN，也不需要RNN。</p><p>在这里呢，我们其实用普通的Dense层，也就是密集连接层，来搭建神经网络就可以了。Dense层是最普通的全连接网络层，因为它其中既没有卷积，也没有循环。而这样的神经网络我们叫它DNN。DNN网络非常适合解决分类问题，尤其是特征比较多的情况。</p><p>那DNN的网络结构是什么样的呢？别着急，我们先把这个模型构建起来，再一探究竟。由于数据集不大，这次实战并不需要GPU的出场，所以，我们直接在Jupyter Notebook上跑神经网络模型就可以了。</p><p>首先，我们安装Keras和Tensorflow这两个神经网络框架：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">!pip install keras</span></div><div class="token-line"><span class="token plain">    !pip install tensorflow</span></div></pre></div><p>安装过程输出如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">Collecting keras</span></div><div class="token-line"><span class="token plain">    ......</span></div><div class="token-line"><span class="token plain">    Successfully installed keras 2.6.0</span></div><div class="token-line"><span class="token plain">    ......</span></div><div class="token-line"><span class="token plain">    Collecting tensorflow</span></div><div class="token-line"><span class="token plain">    Downloading tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)</span></div><div class="token-line"><span class="token plain">    ......</span></div><div class="token-line"><span class="token plain">    Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.</span></div><div class="token-line"><span class="token plain">    ......</span></div></pre></div><p>下面就开始搭建DNN神经网络模型：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">import keras # 导入Keras库</span></div><div class="token-line"><span class="token plain">    from keras.models import Sequential # 导入Keras序贯模型</span></div><div class="token-line"><span class="token plain">    from keras.layers import Dense # 导入Keras密集连接层</span></div><div class="token-line"><span class="token plain">    dnn = Sequential() # 创建一个序贯DNN模型</span></div><div class="token-line"><span class="token plain">    dnn.add(Dense(units=12, input_dim=17, activation = &#x27;relu&#x27;)) # 添加输入层</span></div><div class="token-line"><span class="token plain">    dnn.add(Dense(units=24, activation = &#x27;relu&#x27;)) # 添加隐层</span></div><div class="token-line"><span class="token plain">    dnn.add(Dense(units=1, activation = &#x27;sigmoid&#x27;)) # 添加输出层</span></div><div class="token-line"><span class="token plain">    dnn.summary() # 显示网络模型（这个语句不是必须的）</span></div><div class="token-line"><span class="token plain">    # 编译神经网络，指定优化器，损失函数，以及评估标准</span></div><div class="token-line"><span class="token plain">    dnn.compile(optimizer = &#x27;RMSProp&#x27;, #优化器</span></div><div class="token-line"><span class="token plain">                loss = &#x27;binary_crossentropy&#x27;, #损失函数</span></div><div class="token-line"><span class="token plain">                metrics = [&#x27;acc&#x27;]) #评估标准</span></div></pre></div><p>DNN神经网络的结构输出如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">Model: &quot;sequential&quot;</span></div><div class="token-line"><span class="token plain">    _________________________________________________________________</span></div><div class="token-line"><span class="token plain">    Layer (type)                 Output Shape              Param #   </span></div><div class="token-line"><span class="token plain">    =================================================================</span></div><div class="token-line"><span class="token plain">    dense (Dense)                (None, 12)                228       </span></div><div class="token-line"><span class="token plain">    _________________________________________________________________</span></div><div class="token-line"><span class="token plain">    dense_1 (Dense)              (None, 24)                312       </span></div><div class="token-line"><span class="token plain">    _________________________________________________________________</span></div><div class="token-line"><span class="token plain">    dense_2 (Dense)              (None, 1)                 25        </span></div><div class="token-line"><span class="token plain">    =================================================================</span></div><div class="token-line"><span class="token plain">    Total params: 565</span></div><div class="token-line"><span class="token plain">    Trainable params: 565</span></div><div class="token-line"><span class="token plain">    Non-trainable params: 0</span></div><div class="token-line"><span class="token plain">    _________________________________________________________________</span></div></pre></div><p>可以看到，这个DNN神经网络的结构和我们之前见过的CNN和RNN大同小异，唯一的区别就在于，我们这里使用的是Dense层。</p><p>另外，在编译时，损失函数、优化器和评估指标的选择也不太一样。其中，我们把损失函数指定为binary_crossentropy，这是专门用来二分类问题的损失函数；而对于优化器，我们选择的是RMSProp；评估指标则是Accuracy（分类准确率）。</p><p>如果你仔细看上面搭建DNN网络模型的代码，可能会留意到：前两个Dense层的activation值为relu，而最后一层是sigmoid。这是什么意思呢？</p><p>其实，这里的activation就是神经网络的激活函数。激活函数在神经网络中是用来引入非线性因素的，目的是提升模型的表达能力。否则的话，各层神经元之间只存在线性关系，这种模型的表达能力就不够强，不能覆盖更复杂的特征空间，因此神经网络的每一个神经元，在向下一层网络输出特征时都需要用激活函数进行激活。</p><p>在早期的神经网络中，神经元全都是使用sigmoid函数作为激活函数的，后来又出现了和sigmoid函数类似的tanh函数。不过，人们发现，当输入的特征值较大时，tanh函数的梯度（导数）接近于零，这时参数几乎不再更新，梯度的反向传播过程将被中断，可能会出现梯度消失的现象，而这会影响神经网络的性能。</p><p>这里所说的“反向传播”，就是指在神经网络的梯度下降中，从后面的层向前面层传播的过程（神经网络中，既有从前面层向后面层的传播，也有从后面的层向先前面层的传播）。</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage59bd597a475218f23a9aea9a4350d65c55bd.1b9de282.jpg" alt="" title="常用的神经网络激活函数的函数图像"/></p><p>再后来，人们发现了一个更好的神经元激活函数：ReLU函数。它实现起来非常简单，不仅加速了梯度下降的收敛过程，而且还没有饱和问题，这大大缓解了梯度消失的现象。</p><p>不过，ReLU函数也不是完全没有缺点，在某些情况下，如果参数的权重都处于负值区间，ReLU函数对损失函数的导数可能永远为零。这个神经元将永远不参与整个模型的学习过程，等同于“死掉”。</p><p>为了解决这个问题，人们又发明了Leaky ReLU、eLu、PreLu、Parametric ReLU、Randomized ReLU等变体，为ReLU函数在负区间赋予一定的斜率，从而让它的导数不为零。不过，对于我们初学者来说，不需要深入掌握这些变体的区别和用法，我们一般使用ReLU函数激活就可以了，它也是目前普通神经网络中最常用的激活函数。</p><p>上面所说的激活过程，只是针对于神经网络内部的神经元而言的。而对于神经网络的输出层来说，激活函数的作用就只是确定分类概率了。</p><p>我们知道，概率必须是一个0~1之间的值，这时候，ReLU等函数就无法发挥作用了。所以，如果是二元分类问题，我们在神经网络的输出层中会使用sigmoid函数来进行分类激活；如果是多元分类问题，我们则使用softmax函数进行分类激活。</p><p>搞清楚了DNN网络模型的结构后，现在我们开始进行神经网络的训练。不过，在开始训练之前，我们要做一下格式的转换，把Dataframe格式的对象转换为NumPy张量。关于张量，我们在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/420372">第11讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中介绍过，这里就不重复了。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">X_train = np.asarray(X_train).astype(np.float32) # 转换为NumPy张量</span></div><div class="token-line"><span class="token plain">    X_test = np.asarray(X_test).astype(np.float32) # 转换为NumPy张量</span></div><div class="token-line"><span class="token plain">    history = dnn.fit(X_train, y_train, # 指定训练集</span></div><div class="token-line"><span class="token plain">                  epochs=30,        # 指定训练的轮次</span></div><div class="token-line"><span class="token plain">                  batch_size=64,    # 指定数据批量</span></div><div class="token-line"><span class="token plain">                  validation_split=0.2) #这里直接从训练集数据中拆分验证集，更方便</span></div></pre></div><p>训练过程输出如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">Epoch 1/30</span></div><div class="token-line"><span class="token plain">    71/71 [==============================] - 1s 6ms/step - loss: 4.1202 - acc: 0.6761 - val_loss: 2.9610 - val_acc: 0.4632</span></div><div class="token-line"><span class="token plain">    Epoch 2/30</span></div><div class="token-line"><span class="token plain">    71/71 [==============================] - 0s 3ms/step - loss: 1.1665 - acc: 0.7182 - val_loss: 0.8802 - val_acc: 0.6016</span></div><div class="token-line"><span class="token plain">    Epoch 3/30</span></div><div class="token-line"><span class="token plain">    71/71 [==============================] - 0s 2ms/step - loss: 1.1551 - acc: 0.7087 - val_loss: 2.1645 - val_acc: 0.7773</span></div><div class="token-line"><span class="token plain">    ......</span></div><div class="token-line"><span class="token plain">    Epoch 29/30</span></div><div class="token-line"><span class="token plain">    71/71 [==============================] - 0s 3ms/step - loss: 0.8423 - acc: 0.7495 - val_loss: 1.3655 - val_acc: 0.7862</span></div><div class="token-line"><span class="token plain">    Epoch 30/30</span></div><div class="token-line"><span class="token plain">    71/71 [==============================] - 0s 3ms/step - loss: 0.8477 - acc: 0.7404 - val_loss: 1.1125 - val_acc: 0.7977</span></div></pre></div><p>这个训练信息不够直观，我们要做个处理。不知道你记不记得在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/420372">第11讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中，我们介绍过显示损失曲线和准确率曲线的方法，现在我们就用这个方法看一看在上述训练过程中，随着梯度的下降和模型的拟合，损失和准确率在训练集和验证集上的变化情况：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def show_history(history): # 显示训练过程中的学习曲线</span></div><div class="token-line"><span class="token plain">        loss = history.history[&#x27;loss&#x27;] #训练损失</span></div><div class="token-line"><span class="token plain">        val_loss = history.history[&#x27;val_loss&#x27;] #验证损失</span></div><div class="token-line"><span class="token plain">        epochs = range(1, len(loss) + 1) #训练轮次</span></div><div class="token-line"><span class="token plain">        plt.figure(figsize=(12,4)) # 图片大小</span></div><div class="token-line"><span class="token plain">        plt.subplot(1, 2, 1) #子图1</span></div><div class="token-line"><span class="token plain">        plt.plot(epochs, loss, &#x27;bo&#x27;, label=&#x27;Training loss&#x27;) #训练损失</span></div><div class="token-line"><span class="token plain">        plt.plot(epochs, val_loss, &#x27;b&#x27;, label=&#x27;Validation loss&#x27;) #验证损失</span></div><div class="token-line"><span class="token plain">        plt.title(&#x27;Training and validation loss&#x27;) #图题</span></div><div class="token-line"><span class="token plain">        plt.xlabel(&#x27;Epochs&#x27;) #X轴文字</span></div><div class="token-line"><span class="token plain">        plt.ylabel(&#x27;Loss&#x27;) #Y轴文字</span></div><div class="token-line"><span class="token plain">        plt.legend() #图例</span></div><div class="token-line"><span class="token plain">        acc = history.history[&#x27;acc&#x27;] #训练准确率</span></div><div class="token-line"><span class="token plain">        val_acc = history.history[&#x27;val_acc&#x27;] #验证准确率</span></div><div class="token-line"><span class="token plain">        plt.subplot(1, 2, 2) #子图2</span></div><div class="token-line"><span class="token plain">        plt.plot(epochs, acc, &#x27;bo&#x27;, label=&#x27;Training acc&#x27;) #训练准确率</span></div><div class="token-line"><span class="token plain">        plt.plot(epochs, val_acc, &#x27;b&#x27;, label=&#x27;Validation acc&#x27;) #验证准确率</span></div><div class="token-line"><span class="token plain">        plt.title(&#x27;Training and validation accuracy&#x27;) #图题</span></div><div class="token-line"><span class="token plain">        plt.xlabel(&#x27;Epochs&#x27;) #X轴文字</span></div><div class="token-line"><span class="token plain">        plt.ylabel(&#x27;Accuracy&#x27;) #Y轴文字</span></div><div class="token-line"><span class="token plain">        plt.legend() #图例</span></div><div class="token-line"><span class="token plain">        plt.show() #绘图</span></div><div class="token-line"><span class="token plain">    show_history(history) # 调用这个函数</span></div></pre></div><p>输出如下：</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage602a6081c7da04yy83e55e3dddd7ea13ee2a.8ab40614.png" alt="" title="训练集曲线平滑，但验证集损失曲线和准确率曲线有振荡的情况"/></p><p>从图中可见，训练集的损失逐渐下降，准确率逐渐提升。但是，验证集的曲线不那么漂亮，有振荡的情况，这种情况意味着网络没有训练起来。接下来，我们还是看一看它在测试集上的准确率是否理想吧：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">result = dnn.evaluate(X_test, y_test) #评估测试集上的准确率</span></div><div class="token-line"><span class="token plain">    print(&#x27;DNN的测试准确率为&#x27;,&quot;{0:.2f}%&quot;.format(result[1])*100)</span></div></pre></div><p>输出如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">45/45 [==============================] - 0s 1ms/step - loss: 1.0171 - acc: 0.7658</span></div><div class="token-line"><span class="token plain">    DNN的测试准确率为 77%</span></div></pre></div><p>再看看第一个测试集用户的预测结果：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">prediction = dnn.predict(X_test) #预测测试集的图片分类</span></div><div class="token-line"><span class="token plain">    print(&#x27;第一个用户分类结果为:&#x27;, np.argmax(prediction[0]))</span></div></pre></div><p>输出如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">第一个用户分类结果为: 0</span></div></pre></div><p>0值代表客户未流失，说明这个神经网络模型预测该用户并不会流失，与真值相符。</p><p>那这个模型到底满不满足我们的需求呢？其实，我们还是不能确定。这个问题我们暂且放在一边，后续再做探讨。</p><p>现在，我想请你思考一下，DNN神经网络模型在测试集上的预测准确率达到了77%，表面上还可以。但是，损失曲线和准确率曲线图却显示，这个模型的损失和准确率都出现了很大的振荡波动，时好时坏。这又是什么原因呢？</p><h2 id="归一化之后重新训练神经网络"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#归一化之后重新训练神经网络"><span class="icon icon-link"></span></a>归一化之后重新训练神经网络</h2><p>其实，这种振荡现象的出现是数据所造成的。我们之前说过，神经网络非常不喜欢未经归一化的数据，因此，对于神经网络来说，我们前面对这个数据集做预处理时，可能缺少了一个环节，就是归一化。</p><p>下面，我们就把应该做的对X特征集的归一化工作给补上：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">from sklearn.preprocessing import MinMaxScaler #导入归一化缩放器</span></div><div class="token-line"><span class="token plain">    scaler = MinMaxScaler() #创建归一化缩放器</span></div><div class="token-line"><span class="token plain">    X_train = scaler.fit_transform(X_train) #拟合并转换训练集数据</span></div><div class="token-line"><span class="token plain">    X_test = scaler.transform(X_test) #转换测试集数据</span></div></pre></div><p>然后，我们仍然用同样DNN神经网络训练数据，并绘制损失曲线和准确率曲线：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">history = dnn.fit(X_train, y_train, # 指定训练集</span></div><div class="token-line"><span class="token plain">                      epochs=30,        # 指定训练的轮次</span></div><div class="token-line"><span class="token plain">                      batch_size=64,    # 指定数据批量</span></div><div class="token-line"><span class="token plain">                      validation_split=0.2) #指定验证集,这里为了简化模型，直接用训练集数据</span></div><div class="token-line"><span class="token plain">    show_history(history) # 调用这个函数</span></div></pre></div><p>输出如下：</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage1a7c1ab4e30d68148151abba8209db2cfd7c.172c63d6.png" alt="" title="归一化之后，损失曲线和准确率曲线变得平滑"/></p><p>结果显示，振荡现象消失了，曲线的变得平滑了很多，这是神经网络能够正常训练起来的一种表现：</p><p>最后，我们看一下新的神经网络模型的测试准确率。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">result = dnn.evaluate(X_test, y_test) #评估测试集上的准确率</span></div><div class="token-line"><span class="token plain">    print(&#x27;DNN（归一化之后）的测试准确率为&#x27;,&quot;{0:.2f}%&quot;.format(result[1])*100)</span></div></pre></div><p>输出如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">45/45 [==============================] - 0s 2ms/step - loss: 78.6179 - acc: 0.7800</span></div><div class="token-line"><span class="token plain">    DNN（归一化之后）的测试准确率为 78%</span></div></pre></div><p>可以看到，归一化后的测试准确率为78%，比起刚才的77%，基本没什么差别。</p><p>到这里，我们针对“预测哪些客户流失风险比较高”这个任务，产出了三个模型：逻辑回归模型、未做归一化的神经网络模型DNN，以及归一化之后的神经网络模型DNN。那这三种模型是不是都符合我们的需求呢？哪一种模型更好呢？你可以思考一下，在下一讲中，我会为你揭晓答案。</p><h2 id="总结一下"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#总结一下"><span class="icon icon-link"></span></a>总结一下</h2><p>这节课，我们用逻辑回归和深度学习神经网络预测了“易速鲜花”的会员是否会流失，这是一个典型的二元分类问题。二元分类是很常见的一类监督学习问题，能够用于解决二元分类问题的算法也非常多，包括逻辑回归、朴素贝叶斯、KNN等等。</p><p>逻辑回归是解决二元分类问题最简单的方法，它的实现也比较简单，就是从sklearn中导入、创建并拟合逻辑回归模型，方法与我们在前面几关中的步骤完全相同。</p><p>值得一提的是，在解决二元分类问题时，我们可以通过logreg.predict()函数来预测分类的值，也可以通过logreg.predict_proba()函数来输出分类的概率，概率越高，模型就认为归为该类可能性越大。举例来说，0.51和0.99两个概率值，模型都预测用户会流失，但是有多大的信心？明显概率为0.99的高得多。</p><p>在用神经网络解决二元分类问题时，我们选择了最普通的神经网络模型DNN，它的创建和训练过程与我们在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/420372">第11讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>到<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/422439">第13讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中的步骤并无区别。由于神经网络模型的predict()函数输出的是概率，如果你需要手工进行分类转化，可以用np.argmax函数来完成，这个步骤我们在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/420372">第11讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>也介绍过，你如果不太清楚，可以再复习一下。</p><p>最后，我还想强调一点，通过这一讲，我们已经看到，对于神经网络的输入张量，如果不做归一化，就会影响神经网络的训练效果。因此，我希望你能明白为神经网络输入张量做归一化的重要性。</p><h2 id="思考题"><a aria-hidden="true" tabindex="-1" href="/blog-other/零基础实战机器学习/03.业务场景闯关篇/11#思考题"><span class="icon icon-link"></span></a>思考题</h2><p>好，这节课就到这里，我给你留两道思考题：</p><ol><li>除了逻辑回归和神经网络之外，我们还列出了不少分类算法，你能否尝试使用其它分类算法来解决这个问题？</li><li>请你试着调整DNN神经网络的结构（增加减少层和神经元的个数）、调整编译时的各参数，或者增加减少训练的轮次等，看一看有什么发现。</li></ol><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="/blog-other/static/httpsstatic001geekbangorgresourceimage554255cbce594de6e275f6dce9ddafce6b42.85aef1dc.jpg" alt=""/></p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/零基础实战机器学习/03.业务场景闯关篇/11.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/23 21:58:31</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-other/umi.js"></script>
  </body>
</html>
